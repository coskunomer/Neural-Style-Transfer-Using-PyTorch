{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "608dTn6OCPsw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cGnSoAzaDDef",
    "outputId": "19feefd3-a900-403d-e7c3-0a14b4802658"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MUjC4-PWDJpH"
   },
   "outputs": [],
   "source": [
    "# loading the model vgg19 that will serve as the base model\n",
    "vgg = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qxr0E8aODKRe"
   },
   "outputs": [],
   "source": [
    "def imageLoader(imagePath):\n",
    "    # opening the image\n",
    "    image = Image.open(imagePath)\n",
    "    # resizing the image and converting to tensor\n",
    "    loader = transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
    "    image=loader(image).unsqueeze(0).to(device, )\n",
    "    return image\n",
    "\n",
    "# loading the content and the style images\n",
    "content_images, style_images = [], []\n",
    "for i in range(1, 4):\n",
    "    content_images.append(imageLoader(f\"images/Content images/content_image{i}.jpg\"))\n",
    "    style_images.append(imageLoader(f\"images/Style images/style_image{i}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LwCJqCuRDKv8"
   },
   "outputs": [],
   "source": [
    "# [0,5,10,19,28] are the index of the layers we will be using to calculate the loss as per the paper of NST\n",
    "#Defining a class that for the model\n",
    "\n",
    "class StyleTransferModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "        # Here we will use the following layers and make an array of their indices\n",
    "        self.layers= ['0','5','10','19','28'] \n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(-1, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(-1, 1, 1)\n",
    "        # since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n",
    "        self.model = models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
    "    \n",
    "   \n",
    "    # x holds the input tensor(image) that will be feeded to each layer\n",
    "    def forward(self, img):\n",
    "        # VGG networks are trained on images with each channel normalized by \n",
    "        # mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. We will use \n",
    "        # them to normalize the image before sending it into the network.\n",
    "        img = (img - self.mean) / self.std\n",
    "        # initialize an array that wil hold the activations from the chosen layers\n",
    "        features=[]\n",
    "        # iterate over all the layers of the mode\n",
    "        for layer_num,layer in enumerate(self.model):\n",
    "            # activation of the layer will stored in x\n",
    "            img = layer(img)\n",
    "            # appending the activation of the selected layers and return the feature array\n",
    "            if (str(layer_num) in self.layers):\n",
    "                features.append(img)\n",
    "                \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wOf7ukEjDO6n"
   },
   "outputs": [],
   "source": [
    "def calc_content_loss(generated, original):\n",
    "    # content loss is the MSE between the generated content and actual content\n",
    "    contentLoss = F.mse_loss(generated, original)\n",
    "    return contentLoss\n",
    "\n",
    "def calc_style_loss(generated, style):\n",
    "    # getting the batch size, number of feature maps, height and width \n",
    "    batch_size, n_feature_maps, height, width = generated.shape\n",
    "    # reshaping\n",
    "    features_G = generated.view(batch_size * n_feature_maps, height * width)\n",
    "    # gram matrix\n",
    "    G = torch.mm(features_G, features_G.t())\n",
    "    # getting the batch size, number of feature maps, height and width \n",
    "    batch_size, n_feature_maps, height, width = style.shape\n",
    "    # reshaping\n",
    "    features_A = style.view(batch_size * n_feature_maps, height * width)\n",
    "    # gram matrix\n",
    "    A = torch.mm(features_A, features_A.t())\n",
    "    # calculating the style loss, mse of gram matrices of actual style image and generated image\n",
    "    styleLoss= F.mse_loss(G, A)\n",
    "    return styleLoss\n",
    "\n",
    "def calculate_loss(gen_features, orig_feautes, style_featues, alpha, beta):\n",
    "    styleLoss, contentLoss = 0, 0\n",
    "    for gen, content, style in zip(gen_features,orig_feautes,style_featues):\n",
    "        #extracting the dimensions from the generated image\n",
    "        contentLoss += calc_content_loss(gen, content)\n",
    "        styleLoss += calc_style_loss(gen, style)\n",
    "    \n",
    "    # calculating the total loss of e th epoch\n",
    "    totalLoss = alpha * contentLoss + beta * styleLoss \n",
    "    return totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddtGLfh4DPMU",
    "outputId": "ede172eb-d099-4398-a0a4-c699fa04e3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENT IMAGE 1 STYLE IMAGE 1\n",
      "episode 0, loss = 726358622208.0\n",
      "episode 100, loss = 9133766656.0\n",
      "episode 200, loss = 4170377216.0\n",
      "episode 300, loss = 2415852544.0\n",
      "episode 400, loss = 1666021248.0\n",
      "episode 500, loss = 1271853952.0\n",
      "episode 600, loss = 1027066688.0\n",
      "episode 700, loss = 857640448.0\n",
      "episode 800, loss = 732144704.0\n",
      "episode 900, loss = 635751232.0\n",
      "CONTENT IMAGE 1 STYLE IMAGE 2\n",
      "episode 0, loss = 1076838531072.0\n",
      "episode 100, loss = 24068179968.0\n",
      "episode 200, loss = 8712312832.0\n",
      "episode 300, loss = 4135428864.0\n",
      "episode 400, loss = 2486430720.0\n",
      "episode 500, loss = 1744142848.0\n",
      "episode 600, loss = 1337708800.0\n",
      "episode 700, loss = 1084915200.0\n",
      "episode 800, loss = 915507648.0\n",
      "episode 900, loss = 791683904.0\n",
      "CONTENT IMAGE 1 STYLE IMAGE 3\n",
      "episode 0, loss = 1053593108480.0\n",
      "episode 100, loss = 41669963776.0\n",
      "episode 200, loss = 15588938752.0\n",
      "episode 300, loss = 8232599040.0\n",
      "episode 400, loss = 5958195200.0\n",
      "episode 500, loss = 4788913152.0\n",
      "episode 600, loss = 4014006016.0\n",
      "episode 700, loss = 3443108608.0\n",
      "episode 800, loss = 2994884864.0\n",
      "episode 900, loss = 2632511232.0\n",
      "CONTENT IMAGE 2 STYLE IMAGE 1\n",
      "episode 0, loss = 1631718473728.0\n",
      "episode 100, loss = 61901492224.0\n",
      "episode 200, loss = 16978041856.0\n",
      "episode 300, loss = 10113229824.0\n",
      "episode 400, loss = 7180809216.0\n",
      "episode 500, loss = 5593235968.0\n",
      "episode 600, loss = 4582665728.0\n",
      "episode 700, loss = 3860318976.0\n",
      "episode 800, loss = 3306613760.0\n",
      "episode 900, loss = 2863923968.0\n",
      "CONTENT IMAGE 2 STYLE IMAGE 2\n",
      "episode 0, loss = 1818701070336.0\n",
      "episode 100, loss = 61689102336.0\n",
      "episode 200, loss = 31434303488.0\n",
      "episode 300, loss = 16992234496.0\n",
      "episode 400, loss = 9610524672.0\n",
      "episode 500, loss = 6273191936.0\n",
      "episode 600, loss = 4646436864.0\n",
      "episode 700, loss = 3707375616.0\n",
      "episode 800, loss = 3087576576.0\n",
      "episode 900, loss = 2637983744.0\n",
      "CONTENT IMAGE 2 STYLE IMAGE 3\n",
      "episode 0, loss = 238206926848.0\n",
      "episode 100, loss = 21025771520.0\n",
      "episode 200, loss = 9367907328.0\n",
      "episode 300, loss = 6421073408.0\n",
      "episode 400, loss = 4937233408.0\n",
      "episode 500, loss = 3940199936.0\n",
      "episode 600, loss = 3209870592.0\n",
      "episode 700, loss = 2652979968.0\n",
      "episode 800, loss = 2278127360.0\n",
      "episode 900, loss = 1841889792.0\n",
      "CONTENT IMAGE 3 STYLE IMAGE 1\n",
      "episode 0, loss = 1151645777920.0\n",
      "episode 100, loss = 22903005184.0\n",
      "episode 200, loss = 9922039808.0\n",
      "episode 300, loss = 5832780288.0\n",
      "episode 400, loss = 3909192192.0\n",
      "episode 500, loss = 2801815808.0\n",
      "episode 600, loss = 2105222272.0\n",
      "episode 700, loss = 1643634816.0\n",
      "episode 800, loss = 1326553984.0\n",
      "episode 900, loss = 1101056768.0\n",
      "CONTENT IMAGE 3 STYLE IMAGE 2\n",
      "episode 0, loss = 1207473012736.0\n",
      "episode 100, loss = 36967837696.0\n",
      "episode 200, loss = 17529448448.0\n",
      "episode 300, loss = 10336968704.0\n",
      "episode 400, loss = 6668430336.0\n",
      "episode 500, loss = 4576751616.0\n",
      "episode 600, loss = 3283950336.0\n",
      "episode 700, loss = 2448273152.0\n",
      "episode 800, loss = 1886680192.0\n",
      "episode 900, loss = 1505642496.0\n",
      "CONTENT IMAGE 3 STYLE IMAGE 3\n",
      "episode 0, loss = 322254733312.0\n",
      "episode 100, loss = 23513810944.0\n",
      "episode 200, loss = 11120387072.0\n",
      "episode 300, loss = 6164104704.0\n",
      "episode 400, loss = 3889789696.0\n",
      "episode 500, loss = 2756085760.0\n",
      "episode 600, loss = 2102158592.0\n",
      "episode 700, loss = 1677053056.0\n",
      "episode 800, loss = 1382682624.0\n",
      "episode 900, loss = 1167555456.0\n"
     ]
    }
   ],
   "source": [
    "#initialize the paramerters required for fitting the model\n",
    "epoch=1000\n",
    "learning_rate = 0.005\n",
    "alpha = 8\n",
    "beta = 100\n",
    "counter = 0\n",
    "\n",
    "# iterating over all the content and style images\n",
    "for i, content_image in enumerate(content_images):\n",
    "    for j, style_image in enumerate(style_images):\n",
    "        print(f\"CONTENT IMAGE {i+1} STYLE IMAGE {j+1}\")\n",
    "        counter += 1\n",
    "        # creating the generated image from the original image\n",
    "        generated_image = content_image.clone().requires_grad_(True)\n",
    "\n",
    "        # initializing the model\n",
    "        model=StyleTransferModel().to(device).eval() \n",
    "\n",
    "        #using adam optimizer and it will update the generated image not the model parameter \n",
    "        optimizer = optim.Adam([generated_image], lr=learning_rate)\n",
    "\n",
    "        #iterating for 1000 times\n",
    "        for e in range(epoch):\n",
    "            #extracting the features of generated, content and the original required for calculating the loss\n",
    "            gen_features = model(generated_image)\n",
    "            orig_feautes = model(content_image)\n",
    "            style_featues = model(style_image)\n",
    "            \n",
    "            # iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n",
    "            total_loss = calculate_loss(gen_features, orig_feautes, style_featues, alpha, beta)\n",
    "            # optimize the pixel values of the generated image and backpropagate the loss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print the image and save it after each 100 epoch\n",
    "            if e % 100 == 0:\n",
    "                print(f\"episode {e}, loss = {total_loss}\")        \n",
    "                save_image(generated_image, f\"images/Results/generated{counter}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3aSKG8HHDo6x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
